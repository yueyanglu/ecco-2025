{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f4c5e1-4663-4519-a5ad-ca97c7908320",
   "metadata": {},
   "source": [
    "# Run MPI Hello World Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34d3e9-eca0-4520-b02a-5a7c2eed0e25",
   "metadata": {},
   "source": [
    "This tutorial introduces examples for running a simple MPI \"Hello World\" Fortran code using SLURM, both as an interactive job and as a batch job. \n",
    "\n",
    "```{note}\n",
    "The files mentioned in this tutorial are accessible on the P-Cluster at `/efs_ecco/ECCO/misc/hello/`. Some of them are also provided as downloadable links within this tutorial.\n",
    "``` \n",
    "\n",
    "## MPI Hello World Fortran Code\n",
    "\n",
    "We use an MPI Fortran code called [hello.f](./hello/hello.f) in which each process prints out \"Hello, world!\".\n",
    "\n",
    "## Create a Local Test Directory and Obtain the Relevant Files\n",
    "Users first need to copy the directory `/efs_ecco/ECCO/misc/hello/` to a preferred location (e.g., `/home/USERNAME/mpitest/` in the example below, with `USERNAME` replaced by the actual username), and then change into `~/mpitest/hello/`, as shown in the following example:\n",
    "\n",
    "```\n",
    "echo \"navigate to your home directory\"\n",
    "cd ~\n",
    "\n",
    "echo \"show your home directory \"\n",
    "pwd\n",
    "/home/USERNAME\n",
    "\n",
    "echo \"make a directory for the mpi hello test files\"\n",
    "mkdir mpitest\n",
    "\n",
    "echo \"navigate into that directory\"\n",
    "cd mpitest\n",
    "\n",
    "echo \"copy the hello world mpi files here\"\n",
    "cp -pr /efs_ecco/ECCO/misc/hello .\n",
    "\n",
    "echo \"navigate  into that directory\"\n",
    "cd hello\n",
    "\n",
    "echo \"show contents\n",
    "ls -1\n",
    "```\n",
    "Users should see the following:\n",
    "```\n",
    "README\n",
    "hello\n",
    "hello.f\n",
    "hello_job_1140.log\n",
    "hello_salloc.log\n",
    "hello_sbatch.csh\n",
    "```\n",
    "\n",
    "## Compile\n",
    "The executable, called `hello`, has been pre-gerenated. If needed, use the following command to re-compile the Fortran program `hello.f` and generate the executable:\n",
    "```\n",
    "mpiifort -O2 -o hello hello.f\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f38973-476b-463a-97ab-cd762eef6b63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> The \"O\" in -O2 is  letter \"O\", not Zero. `-O2` stands for Optimization level 2.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48aa1e2",
   "metadata": {},
   "source": [
    "\n",
    "## Run as an Interactive Job\n",
    "\n",
    "To run the executable as an interactive job using 6 CPUs, use the following command to request the computing resoure:\n",
    "```\n",
    "salloc --ntasks=6 --ntasks-per-node=2 --partition=sealevel-c5xl-demand --time=00:05:00\n",
    "```\n",
    "\n",
    "Here, `salloc` requests 6 tasks with 2 tasks per node, effectively requesting 3 nodes from the partition `sealevel-c5xl-demand`. We only request 5 minute of wall clock time (`--time=00:05:00`) since the job runs quickly (the actual run completes in at most a couple of seconds). Although the job itself finishes quickly, it often takes a few minutes for the computing resources to become ready. During this period, users will see a message similar to the following:\n",
    "```\n",
    "salloc: Granted job allocation 1263\n",
    "salloc: Waiting for resource configuration\n",
    "```\n",
    "When the allocation is granted, users will see the following message, and the prompt will appear:\n",
    "```\n",
    "salloc: Nodes sealevel-c5xl-demand-dy-c5xlarge-[1-3] are ready for job\n",
    "```\n",
    "Users can then use the following command to use of the node as the inactive node (replace the job ID `1263` with the actual job ID):\n",
    "```\n",
    "srun --jobid=1263 --pty /bin/bash\n",
    "```\n",
    "Uses will notice the prompt changes \n",
    "```\n",
    "USERNAME@sealevel-c5xl-demand-dy-c5xlarge-1:/SOMEWORKINGDIR/$\n",
    "```\n",
    "Then, issue the following command to launch the executable `./hello` using 6 procrosses. \n",
    "```\n",
    "mpirun -np 6 ./hello\n",
    "```\n",
    "\n",
    "The output of the interactive job is as follows:\n",
    "```\n",
    "salloc: Granted job allocation 1139\n",
    "salloc: Waiting for resource configuration\n",
    "salloc: Nodes sealevel-c5xl-demand-dy-c5xlarge-[1-3] are ready for job\n",
    "  Process   1 says \"Hello, world!\"\n",
    "12 May       2025  10:08:00.038 PM      \n",
    "\n",
    "HELLO_MPI - Master process:\n",
    "  FORTRAN77/MPI version\n",
    "  A simple MPI program.\n",
    "\n",
    "  The number of processes is   6\n",
    "\n",
    "  Process   0 says \"Hello, world!\"\n",
    "\n",
    "Elapsed wall clock time =   0.109248E-04 seconds.\n",
    "  Process   2 says \"Hello, world!\"\n",
    "  Process   3 says \"Hello, world!\"\n",
    "  Process   4 says \"Hello, world!\"\n",
    "  Process   5 says \"Hello, world!\"\n",
    "\n",
    "HELLO_MPI:\n",
    "  Normal end of execution.\n",
    "\n",
    "12 May       2025  10:08:00.062 PM      \n",
    "salloc: Relinquishing job allocation 1139\n",
    "```\n",
    "\n",
    "There are 6 printouts of \"Hello, world!\", corresponding to the 6 processors.\n",
    "\n",
    "Issue the `exit` command times (see explanations below) and release the allocated resources:\n",
    "```\n",
    "exit\n",
    "exit\n",
    "```\n",
    "The first `exit` exits the interactive shell started by the `srun` command, while the second `exit` ends the `salloc` session and releases the allocated resources. After the first \"exit\", you will see the prompt changes back to `USERNAME@ip-10-20-22-69:~$`. After the second `exit`, you will see the following message: \n",
    "```\n",
    "scsalloc: Relinquishing job allocation 1139\n",
    "```\n",
    "\n",
    "If you now issue the command `squeue`, you will see your job ID is not longer in the job list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b227c1-222e-47f3-beda-995544f8b7d6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> If you look carefully, you'll see that the lines with 'Process  n says \"Hello, world!\" can be out of order (really scrambled) because how *nix machines flush the caches of text sent to STDOUT. It's not a big deal if the order of the 'Process  n says \"Hello World\"' is scrambled. If you see Hello Worlds!, you're good.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930241b3-3cf0-4d95-8716-5897585ff262",
   "metadata": {},
   "source": [
    "## Run as a Batch Job\n",
    "\n",
    "We can also run the executable as a batch job. To do this, we create a shell script called [hello_sbatch.csh](./hello/hello_sbatch.csh) to run the executable using 12 processors as a batch job. To submit it, issue the following command:\n",
    "```\n",
    "sbatch hello_sbatch.csh\n",
    "```\n",
    "\n",
    "The script first specifies some SLURM directives, such as `#SBATCH -J hello_sbatch`, which define the job submission parameters and are interpreted by SLURM when you submit the script using `sbatch`. It then sets the number of processors (`nprocs`) and use `mpirun` to lauch the executable. \n",
    "\n",
    "The output will be written to a file called `hello_job_NNNN.log` (where `NNNN` is the actual job number) in the directory where the job is submitted. You should see 12 printouts of \"Hello, world!\" in the `hello_job_NNNN.log` file. An example output file from the batch job is as follows:\n",
    "```\n",
    "  Process   1 says \"Hello, world!\"\n",
    "  Process   2 says \"Hello, world!\"\n",
    "  Process   3 says \"Hello, world!\"\n",
    "  Process   4 says \"Hello, world!\"\n",
    "  Process   5 says \"Hello, world!\"\n",
    "13 May       2025   0:09:28.738 AM\n",
    "\n",
    "HELLO_MPI - Master process:\n",
    "  FORTRAN77/MPI version\n",
    "  A simple MPI program.\n",
    "\n",
    "  The number of processes is  12\n",
    "\n",
    "  Process   0 says \"Hello, world!\"\n",
    "\n",
    "Elapsed wall clock time =   0.138135E-05 seconds.\n",
    "  Process   6 says \"Hello, world!\"\n",
    "  Process   7 says \"Hello, world!\"\n",
    "  Process   8 says \"Hello, world!\"\n",
    "  Process   9 says \"Hello, world!\"\n",
    "  Process  10 says \"Hello, world!\"\n",
    "  Process  11 says \"Hello, world!\"\n",
    "\n",
    "HELLO_MPI:\n",
    "  Normal end of execution.\n",
    "\n",
    "13 May       2025   0:09:28.824 AM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beffb96-d1de-4e05-9aea-c5a6034e82e6",
   "metadata": {},
   "source": [
    "See, here the Hello, world! statements are also jumbled up.  No big deal, delcare success and move on."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
